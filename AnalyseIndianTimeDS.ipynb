{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "   This project is to analyse the time spend in house holds so the government organizations can better understand the popular activities among the people of different states and utilize the people accordingly\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "This project specifically used to analyse the time spent by each person in the houses of 6 states and identify which activities performed by them. With this analysis the government organization can get various KPI's such that what the popular activity in each states,How much time spent on each activity, what factors effecting the activities between states. For ex: Lets take an activity as \"Fetching Of Water\". In the state of Tamilnadu the time taken is 5 min where as in the state of Haryana it's 1 hour. So with this data the Government Organisations can asses why it is taking time in Haryand and take the necessary action to reduce the time. \\\n",
    "\n",
    "The end solution for this project is to get the key activities that is happening in each house holds and how it is effecting the livelihood of life among various states.With this the Government organisations can take necessary steps to give better life to the people by reducing the complexity of certain activities like \"Fetching of Water\" by planning some canals/wells in the nearby areas.\n",
    "\n",
    "Tools used are as follows :\n",
    "\n",
    "        1. Amazon s3 for storage\n",
    "        2. Amazon EMR with Spark for analysis\n",
    "        3. The data will be read from s3 storage from CSV files converted into Parquest file and again saved to S3 for further analysis\n",
    "        4. PowerBI for Dashboard with various metrics\n",
    "\n",
    "#### Describe and Gather Data \n",
   
    "As the part of this project the data which will be explored is from https://www.kaggle.com/arjunprasadsarkhel/indian-time-use-survey?select=TimeUse-India.csv.\n",
    "\n",
    "This data has information regarding various activities performed in the Indian households on different states/districts.There are 3 parts of this data as following:\n",
    "\n",
    "        1. Acitivity Codes - which describes the Acitivity\n",
    "        2. Different Indian states.\n",
    "        3. The collection of data regarding who are the households and what are the activities being performed and also informations like\n",
    "            which state they belong,duration etc. \n",
    "            \n",
    "The TimeUse-India has **3.3M** rows that will be analysed via Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "'''Read the activity CSV from the s3 storage and save it in a Dataframe'''\n",
    "activity_code_schema = StructType([\n",
    "    StructField(\"activity_code\",IntegerType()),\n",
    "    StructField(\"activity\",StringType())    \n",
    "]) \n",
    "\n",
    "activity_df= spark.read.csv(\"s3a://indiantimedataset/DataSet/AcitivityCodes.csv\",schema=activity_code_schema,header=True)\n",
    "activity_df.printSchema()\n",
    "activity_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "+-------------+--------------------+\n",
    "|activity_code|            activity|\n",
    "+-------------+--------------------+\n",
    "|          111| Ploughing, prepa...|\n",
    "|          112| Sewing. planting...|\n",
    "|          113| Application of m...|\n",
    "|          114|            Weeding |\n",
    "|          115| Supervision of w...|\n",
    "+-------------+--------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''Read the state code CSV and save it in the Dataframe'''\n",
    "\n",
    "state_code_schema = StructType([\n",
    "    StructField('state',StringType()),\n",
    "    StructField('state_code',IntegerType())\n",
    "])\n",
    "state_df= spark.read.csv(\"s3a://indiantimedataset/DataSet/StateCodes.csv\",header=True,schema=state_code_schema)\n",
    "state_df.printSchema()\n",
    "state_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "+----------------+----------+\n",
    "|           state|state_code|\n",
    "+----------------+----------+\n",
    "|        Haryana |         1|\n",
    "| Madhya Pradesh |         2|\n",
    "|        Gujarat |         3|\n",
    "|          Orissa|         4|\n",
    "|      Tamil Nadu|         5|\n",
    "+----------------+----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "'''Read the Time Used CSV CSV and save it in the Dataframe'''\n",
    "\n",
    "activities_schema = StructType([\n",
    "    StructField('_c0',StringType()),\n",
    "     StructField('Key_membno',StringType()),\n",
    "     StructField('Key_hhold',StringType()),\n",
    "     StructField('Rec_id',StringType()),\n",
    "     StructField('Schedule_ID',StringType()),\n",
    "     StructField('Schedule_No',IntegerType()),\n",
    "     StructField('sector',IntegerType()),\n",
    "     StructField('subround',IntegerType()),\n",
    "     StructField('subsample',IntegerType()),\n",
    "     StructField('State',IntegerType()),\n",
    "     StructField('District',IntegerType()),\n",
    "     StructField('district_class',IntegerType()),\n",
    "     StructField('Tehsil_Town',IntegerType()),\n",
    "     StructField('stratum',IntegerType()),\n",
    "     StructField('Vill_Blk',IntegerType()),\n",
    "     StructField('sub_blk',IntegerType()),\n",
    "     StructField('stage2_stratum',IntegerType()),\n",
    "     StructField('Hholdno',IntegerType()),\n",
    "     StructField('membno',IntegerType()),\n",
    "    StructField('age',IntegerType()),\n",
    "    StructField(\"type_of_day\",IntegerType()),\n",
    "    StructField(\"hour_serial_no\",IntegerType()),\n",
    "    StructField(\"activity_serial_no\",IntegerType()),\n",
    "    StructField(\"hour_from\",IntegerType()),\n",
    "    StructField(\"hour_to\",IntegerType()),\n",
    "     StructField(\"multiple_activity\",IntegerType()),\n",
    "     StructField(\"actual_time_spent_minutes\",IntegerType()),\n",
    "     StructField(\"within_outside_household\",IntegerType()),\n",
    "    StructField(\"activity\",IntegerType()),   \n",
    "    \n",
    "])\n",
    "\n",
    "activities_df = spark.read.csv(\"s3a://indiantimedataset/DataSet/time_use_partial.csv\",header=True,schema=activities_schema)\n",
    "activities_df = activities_df.select('Key_membno','Key_hhold','State','District','district_class','age','type_of_day','hour_from','hour_to',\n",
    "                                  'multiple_activity','actual_time_spent_minutes','within_outside_household','activity')\n",
    "\n",
    "activities_df.printSchema()\n",
    "activities_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "activities_df.write.partitionBy('state','age','activity').parquet(\"s3a://indiantimedataset/output/time_use_analysis_part.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "#### Cleaning Steps\n",
    "Since the analysis is heavily dependent on the key data such as Age,Activity and State these should checked to have proper values as following.\n",
    "\n",
    "    1. Age should not be equal to 0 and between 0 and 100\n",
    "    2. State cannot be 0\n",
    "    3. activity cannot be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "activities_df = activities_df.filter((activities_df['age'] != 0 & (activities_df['age']>0 &activities_df['age']<100)) & (activities_df['state'] != 0) | (activities_df['state'] != None) \n",
    "                                     & (activities_df['activity'] != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "Attached document DataModel.pdf\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Data analysis\n",
    "\n",
    "'''Create a table view for all the three DataFrames to perform query on them and to do the required analyzes. This will be helpful to \n",
    " extract the required data ouf it and store it a Fact Table'''\n",
    "\n",
    "time_use_df= spark.read.parquet(\"s3a://indiantimedataset/output/time_use_analysis.parquet\");\n",
    "\n",
    "time_use_df.createOrReplaceTempView(\"timeuse\")\n",
    "state_df.createOrReplaceTempView(\"state\")\n",
    "activity_df.createOrReplaceTempView(\"activitycode\")\n",
    "\n",
    "#get actual state name and activity name along with code\n",
    "time_use_data = spark.sql(\"\"\"select *,s.State state_name,a.Activity as activity_name from timeuse t inner join activitycode a on t.activity = a.activity_code \\\n",
    "                             inner join state s on s.state_code = t.State\"\"\")\n",
    "\n",
    "\n",
    "time_use_data.spark.parquet(\"s3a://indiantimedataset/output/time_use_analysis_data.parquet\")\n",
    "time_use_data.show(5)\n",
    "time_use_data.createOrReplaceTempView(\"time_use_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "+----------+-----------+--------+--------------+-----------+---------+-------+-----------------+-------------------------+------------------------+-----+---+--------+-------------+--------------------+----------+----------+----------+--------------------+\n",
    "|Key_membno|  Key_hhold|District|district_class|type_of_day|hour_from|hour_to|multiple_activity|actual_time_spent_minutes|within_outside_household|state|age|activity|activity_code|            activity|     state|state_code|state_name|       activity_name|\n",
    "+----------+-----------+--------+--------------+-----------+---------+-------+-----------------+-------------------------+------------------------+-----+---+--------+-------------+--------------------+----------+----------+----------+--------------------+\n",
    "| 1.422E+12|14220001102|       2|             1|          1|        4|      5|                2|                       60|                       1|    6| 40|     911|          911|  Night sleep/ess...| Meghalaya|         6| Meghalaya|  Night sleep/ess...|\n",
    "| 1.422E+12|14220001102|       2|             1|          1|        5|      6|                2|                       60|                       1|    6| 40|     911|          911|  Night sleep/ess...| Meghalaya|         6| Meghalaya|  Night sleep/ess...|\n",
    "| 1.422E+12|14220001102|       2|             1|          2|        4|      5|                2|                       60|                       1|    6| 40|     911|          911|  Night sleep/ess...| Meghalaya|         6| Meghalaya|  Night sleep/ess...|\n",
    "| 1.422E+12|14220001102|       2|             1|          2|        5|      6|                2|                       30|                       1|    6| 40|     911|          911|  Night sleep/ess...| Meghalaya|         6| Meghalaya|  Night sleep/ess...|\n",
    "| 1.422E+12|14220001102|       2|             1|          1|        3|      4|                2|                       60|                       1|    6| 40|     911|          911|  Night sleep/ess...| Meghalaya|         6| Meghalaya|  Night sleep/ess...|\n",
    "+----------+-----------+--------+--------------+-----------+---------+-------+-----------------+-------------------------+------------------------+-----+---+--------+-------------+--------------------+----------+----------+----------+--------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Since the data will be saved into the S3 storage in Parquet format with analysed data the following Quality checks are applicable:\n",
    "\n",
    " 1. The data should not be empty\n",
    " 2. All the state column should have state name value\n",
    " 3. All the Activities will have proper activity name\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_use_analysis_df =  spark.read.parquet(\"s3a://indiantimedataset/output/time_use_analysis_data.parquet\");\n",
    "\n",
    "if time_use_analysis_df.count() > 0:\n",
    "    raise ValueError(\"Data is empty\")\n",
    "\n",
    "time_use_analysis_check_state = time_use_analysis_df.filter(time_use_analysis_df['state'] == 0)\n",
    "    \n",
    "if time_use_analysis_check_state.count() > 0:\n",
    "    raise ValueError(\"State value is missing for some records\")\n",
    "    \n",
    "time_use_analysis_check_activity = time_use_analysis_df.filter(time_use_analysis_df['activity'] == 0)\n",
    "\n",
    "if time_use_analysis_check_activity.count() > 0:\n",
    "    raise ValueError(\"Activity is missing for some rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Sample output when data is empty :\n",
    "    \n",
    "An error was encountered:\n",
    "Data is empty\n",
    "Traceback (most recent call last):\n",
    "ValueError: Data is empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "The technologies used are as follows :\n",
    "    \n",
    "    1. AWS S3 for storage\n",
    "    2. EMR with Spark for analysis\n",
    "    3. PowerBI for dashboard\n",
    "    \n",
    "AWS S3 is the popular and cheap storage compared to Datawarehouse like Redshift, where it can contain any amount of data and need not to be worried about capacity. \n",
    "So for this analysis this should enough to do the analysis. Also Spark has inbuilt functions to read data from S3. Spark uses Parallelization and RDD to store the data\n",
    "and it is popular tool to analyze the data in 100x speed thatn Hadoop. EMR is the offering from AWS where it gives VM's with preconfigured Spark, so that we can use it\n",
    "to run analysis. PowerBI is a popular tool to create dash with various kind of charts for the data that will the Visual representation of the kPIs.\n",
    "\n",
    "The Data should be updated once in every 3 month so that the Government Organisation can ascertain the improvement activities that are happening in the required\n",
    "state/districts. \n",
    "\n",
    "If the data is increased by 100X :\n",
    "    \n",
    "Spark will still be used for Data Manipulation process. Since spark automatically partition the data on the cores available and it is \n",
    "10x faster than Hadoop also doing in-memomry processing the data will still be manipulated using spark.\n",
    "\n",
    "The Apache Airflow can be used to schedule the tasks at 7 am daily run and the data can be processed by splitting across year and month.\n",
    "\n",
    " dag = DAG(\n",
    "        dag_id=\"generated_dag_id\",\n",
    "        start_date=(2021,07,15,07,00,00)\n",
    "        schedule_interval=\"@daily\"\n",
    "    )\n",
    "\n",
    "In order to support 100+ concurrent users rather than storage,Datawarehouse like Redshift would be better option to store since it can support data upto petabytes and \n",
    "having auto scalable options. The replication can also been turned on to support the users across regions and to avoid any data loss.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Sample Queries and Results :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get unique activities on each state\n",
    "unique_activity_per_state = spark.sql(\"\"\"select state_name,activity_name from time_use_data group by state_name,activity_name\"\"\")\n",
    "unique_activity_per_state.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "+----------+--------------------+\n",
    "|state_name|       activity_name|\n",
    "+----------+--------------------+\n",
    "| Meghalaya| Ploughing, prepa...|\n",
    "| Meghalaya|  Assisting depen...|\n",
    "| Meghalaya|  Attendance at p...|\n",
    "| Meghalaya| Collection of ra...|\n",
    "| Meghalaya|  Night sleep/ess...|\n",
    "+----------+--------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get the popular activity\n",
    "popular_activity =spark.sql(\"\"\"select activity_name,count(activity_name) activity_count from time_use_data group by activity_name order by activity_count desc limit 1\"\"\")\n",
    "popular_activity.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "+--------------------+--------------+\n",
    "|       activity_name|activity_count|\n",
    "+--------------------+--------------+\n",
    "|  Night sleep/ess...|           425|\n",
    "+--------------------+--------------+"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
