{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e795b5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644c083b63794d70879b1f8c759431d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- activity_code: integer (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      "\n",
      "+-------------+--------------------+\n",
      "|activity_code|            activity|\n",
      "+-------------+--------------------+\n",
      "|          111| Ploughing, prepa...|\n",
      "|          112| Sewing. planting...|\n",
      "|          113| Application of m...|\n",
      "|          114|            Weeding |\n",
      "|          115| Supervision of w...|\n",
      "+-------------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "activity_code_schema = StructType([\n",
    "    StructField(\"activity_code\",IntegerType()),\n",
    "    StructField(\"activity\",StringType())    \n",
    "]) \n",
    "\n",
    "activity_df= spark.read.csv(\"s3a://indiantimedataset/DataSet/AcitivityCodes.csv\",schema=activity_code_schema,header=True)\n",
    "activity_df.printSchema()\n",
    "activity_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b6bc155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6db53182b834f568f0d22225d0bfb4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state: string (nullable = true)\n",
      " |-- state_code: integer (nullable = true)\n",
      "\n",
      "+----------------+----------+\n",
      "|           state|state_code|\n",
      "+----------------+----------+\n",
      "|        Haryana |         1|\n",
      "| Madhya Pradesh |         2|\n",
      "|        Gujarat |         3|\n",
      "|          Orissa|         4|\n",
      "|      Tamil Nadu|         5|\n",
      "+----------------+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "state_code_schema = StructType([\n",
    "    StructField('state',StringType()),\n",
    "    StructField('state_code',IntegerType())\n",
    "])\n",
    "state_df= spark.read.csv(\"s3a://indiantimedataset/DataSet/StateCodes.csv\",header=True,schema=state_code_schema)\n",
    "state_df.printSchema()\n",
    "state_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "290f0303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcfb29218fc43d98a2e40e2fb7a8d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Key_membno: string (nullable = true)\n",
      " |-- Key_hhold: string (nullable = true)\n",
      " |-- State: integer (nullable = true)\n",
      " |-- District: integer (nullable = true)\n",
      " |-- district_class: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- type_of_day: integer (nullable = true)\n",
      " |-- hour_from: integer (nullable = true)\n",
      " |-- hour_to: integer (nullable = true)\n",
      " |-- multiple_activity: integer (nullable = true)\n",
      " |-- actual_time_spent_minutes: integer (nullable = true)\n",
      " |-- within_outside_household: integer (nullable = true)\n",
      " |-- activity: integer (nullable = true)\n",
      "\n",
      "+----------+-----------+-----+--------+--------------+---+-----------+---------+-------+-----------------+-------------------------+------------------------+--------+\n",
      "|Key_membno|  Key_hhold|State|District|district_class|age|type_of_day|hour_from|hour_to|multiple_activity|actual_time_spent_minutes|within_outside_household|activity|\n",
      "+----------+-----------+-----+--------+--------------+---+-----------+---------+-------+-----------------+-------------------------+------------------------+--------+\n",
      "| 1.422E+12|14220001101|    6|       2|             1| 34|          2|        6|      7|                2|                       60|                       2|     141|\n",
      "| 1.422E+12|14220001101|    6|       2|             1| 34|          1|        7|      8|                2|                       60|                       2|     144|\n",
      "| 1.422E+12|14220001101|    6|       2|             1| 34|          1|        2|      3|                2|                       60|                       2|     321|\n",
      "| 1.422E+12|14220001101|    6|       2|             1| 34|          1|       11|     12|                2|                       60|                       2|     321|\n",
      "| 1.422E+12|14220001101|    6|       2|             1| 34|          1|        3|      4|                2|                       60|                       2|     321|\n",
      "+----------+-----------+-----+--------+--------------+---+-----------+---------+-------+-----------------+-------------------------+------------------------+--------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "activities_schema = StructType([\n",
    "    StructField('_c0',StringType()),\n",
    "     StructField('Key_membno',StringType()),\n",
    "     StructField('Key_hhold',StringType()),\n",
    "     StructField('Rec_id',StringType()),\n",
    "     StructField('Schedule_ID',StringType()),\n",
    "     StructField('Schedule_No',IntegerType()),\n",
    "     StructField('sector',IntegerType()),\n",
    "     StructField('subround',IntegerType()),\n",
    "     StructField('subsample',IntegerType()),\n",
    "     StructField('State',IntegerType()),\n",
    "     StructField('District',IntegerType()),\n",
    "     StructField('district_class',IntegerType()),\n",
    "     StructField('Tehsil_Town',IntegerType()),\n",
    "     StructField('stratum',IntegerType()),\n",
    "     StructField('Vill_Blk',IntegerType()),\n",
    "     StructField('sub_blk',IntegerType()),\n",
    "     StructField('stage2_stratum',IntegerType()),\n",
    "     StructField('Hholdno',IntegerType()),\n",
    "     StructField('membno',IntegerType()),\n",
    "    StructField('age',IntegerType()),\n",
    "    StructField(\"type_of_day\",IntegerType()),\n",
    "    StructField(\"hour_serial_no\",IntegerType()),\n",
    "    StructField(\"activity_serial_no\",IntegerType()),\n",
    "    StructField(\"hour_from\",IntegerType()),\n",
    "    StructField(\"hour_to\",IntegerType()),\n",
    "     StructField(\"multiple_activity\",IntegerType()),\n",
    "     StructField(\"actual_time_spent_minutes\",IntegerType()),\n",
    "     StructField(\"within_outside_household\",IntegerType()),\n",
    "    StructField(\"activity\",IntegerType()),   \n",
    "    \n",
    "])\n",
    "\n",
    "activities_df = spark.read.csv(\"s3a://indiantimedataset/DataSet/time_use_partial.csv\",header=True,schema=activities_schema)\n",
    "activities_df = activities_df.select('Key_membno','Key_hhold','State','District','district_class','age','type_of_day','hour_from','hour_to',\n",
    "                                  'multiple_activity','actual_time_spent_minutes','within_outside_household','activity')\n",
    "\n",
    "activities_df.printSchema()\n",
    "activities_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2bc37f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c582b54d44ef419cba8388e8d5e231ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o236.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:174)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:173)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:112)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:291)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:586)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: No space available in any of the local directories.\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:400)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:461)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:200)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:501)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AOutputStream.<init>(S3AOutputStream.java:66)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:690)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1075)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1056)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:945)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:933)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:433)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:371)\n",
      "\tat com.amazon.emr.committer.FilterParquetOutputCommitter.commitJob(FilterParquetOutputCommitter.java:82)\n",
      "\tat com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter.commitJob(EmrOptimizedSparkSqlParquetOutputCommitter.java:9)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SQLEmrOptimizedCommitProtocol.commitJob(SQLEmrOptimizedCommitProtocol.scala:121)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:187)\n",
      "\t... 37 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 844, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o236.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:174)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:173)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:112)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:291)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:586)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: No space available in any of the local directories.\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:400)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:461)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:200)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:501)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AOutputStream.<init>(S3AOutputStream.java:66)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:690)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1075)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1056)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:945)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:933)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:433)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:371)\n",
      "\tat com.amazon.emr.committer.FilterParquetOutputCommitter.commitJob(FilterParquetOutputCommitter.java:82)\n",
      "\tat com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter.commitJob(EmrOptimizedSparkSqlParquetOutputCommitter.java:9)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SQLEmrOptimizedCommitProtocol.commitJob(SQLEmrOptimizedCommitProtocol.scala:121)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:187)\n",
      "\t... 37 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data cleaning\n",
    "activities_df = activities_df.filter((activities_df['age'] != 0) & (activities_df['state'] != 0) | (activities_df['state'] != None) \n",
    "                                     & (activities_df['activity'] != 0))\n",
    "\n",
    "activities_df.write.partitionBy('state','age','activity').parquet(\"s3a://indiantimedataset/output/time_use_analysis_part.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ad25409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92225be120d0468098ce0ff0f991f969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+--------------+-----------+---------+-------+-----------------+-------------------------+------------------------+-----+---+--------+-------------+--------------------+----------+----------+----------+--------------------+\n",
      "|Key_membno|  Key_hhold|District|district_class|type_of_day|hour_from|hour_to|multiple_activity|actual_time_spent_minutes|within_outside_household|state|age|activity|activity_code|            activity|     state|state_code|state_name|       activity_name|\n",
      "+----------+-----------+--------+--------------+-----------+---------+-------+-----------------+-------------------------+------------------------+-----+---+--------+-------------+--------------------+----------+----------+----------+--------------------+\n",
      "| 1.422E+12|14220001102|       2|             1|          1|        4|      5|                2|                       60|                       1|    6| 40|     911|          911|  Night sleep/ess...| Meghalaya|         6| Meghalaya|  Night sleep/ess...|\n",
      "| 1.422E+12|14220001102|       2|             1|          1|        5|      6|                2|                       60|                       1|    6| 40|     911|          911|  Night sleep/ess...| Meghalaya|         6| Meghalaya|  Night sleep/ess...|\n",
      "| 1.422E+12|14220001102|       2|             1|          2|        4|      5|                2|                       60|                       1|    6| 40|     911|          911|  Night sleep/ess...| Meghalaya|         6| Meghalaya|  Night sleep/ess...|\n",
      "| 1.422E+12|14220001102|       2|             1|          2|        5|      6|                2|                       30|                       1|    6| 40|     911|          911|  Night sleep/ess...| Meghalaya|         6| Meghalaya|  Night sleep/ess...|\n",
      "| 1.422E+12|14220001102|       2|             1|          1|        3|      4|                2|                       60|                       1|    6| 40|     911|          911|  Night sleep/ess...| Meghalaya|         6| Meghalaya|  Night sleep/ess...|\n",
      "+----------+-----------+--------+--------------+-----------+---------+-------+-----------------+-------------------------+------------------------+-----+---+--------+-------------+--------------------+----------+----------+----------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "\n",
    "#Data analysis\n",
    "\n",
    "time_use_df= spark.read.parquet(\"s3a://indiantimedataset/output/time_use_analysis_part.parquet\");\n",
    "\n",
    "time_use_df.createOrReplaceTempView(\"timeuse\")\n",
    "state_df.createOrReplaceTempView(\"state\")\n",
    "activity_df.createOrReplaceTempView(\"activitycode\")\n",
    "\n",
    "#get actual state name and activity name along with code\n",
    "time_use_data = spark.sql(\"\"\"select *,s.State state_name,a.Activity as activity_name from timeuse t inner join activitycode a on t.activity = a.activity_code \\\n",
    "                             inner join state s on s.state_code = t.State\"\"\")\n",
    "time_use_data.show(5)\n",
    "time_use_data.createOrReplaceTempView(\"time_use_data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ce91797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0734284f4f430db7466abf42073295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|state_name|       activity_name|\n",
      "+----------+--------------------+\n",
      "| Meghalaya| Ploughing, prepa...|\n",
      "| Meghalaya|  Assisting depen...|\n",
      "| Meghalaya|  Attendance at p...|\n",
      "| Meghalaya| Collection of ra...|\n",
      "| Meghalaya|  Night sleep/ess...|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "#get unique activities on each state\n",
    "unique_activity_per_state = spark.sql(\"\"\"select state_name,activity_name from time_use_data group by state_name,activity_name\"\"\")\n",
    "unique_activity_per_state.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98c127b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9836ad10154ee19bfeabf23284643b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|       activity_name|activity_count|\n",
      "+--------------------+--------------+\n",
      "|  Night sleep/ess...|           425|\n",
      "+--------------------+--------------+"
     ]
    }
   ],
   "source": [
    "#get the popular activity\n",
    "popular_activity =spark.sql(\"\"\"select activity_name,count(activity_name) activity_count from time_use_data group by activity_name order by activity_count desc limit 1\"\"\")\n",
    "popular_activity.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ba35a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
